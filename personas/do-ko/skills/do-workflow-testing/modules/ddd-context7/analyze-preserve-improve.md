# ANALYZE-PRESERVE-IMPROVE DDD ì‚¬ì´í´

> ëª¨ë“ˆ: Context7 í†µí•©ì„ ê°–ì¶˜ í•µì‹¬ DDD ì‚¬ì´í´ êµ¬í˜„
> ë³µì¡ë„: ê³ ê¸‰
> ì†Œìš” ì‹œê°„: 20ë¶„ ì´ìƒ
> ì˜ì¡´ì„±: Python 3.8+, pytest, Context7 MCP, unittest

## í•µì‹¬ DDD í´ë˜ìŠ¤

```python
import pytest
import unittest
import asyncio
import subprocess
import os
import sys
import time
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum
import json
from pathlib import Path

class DDDPhase(Enum):
    """DDD ì‚¬ì´í´ ë‹¨ê³„."""
    ANALYZE = "analyze"       # ê¸°ì¡´ ì½”ë“œì™€ ë™ì‘ ë¶„ì„
    PRESERVE = "preserve"     # íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ ìƒì„±
    IMPROVE = "improve"       # í…ŒìŠ¤íŠ¸ë¥¼ ë…¹ìƒ‰ìœ¼ë¡œ ìœ ì§€í•˜ë©° ì½”ë“œ ê°œì„ 
    REVIEW = "review"         # ê²€í†  ë° ë³€ê²½ì‚¬í•­ ì»¤ë°‹

class TestType(Enum):
    """DDDì—ì„œ ì‚¬ìš©í•˜ëŠ” í…ŒìŠ¤íŠ¸ ìœ í˜•."""
    UNIT = "unit"
    INTEGRATION = "integration"
    CHARACTERIZATION = "characterization"
    ACCEPTANCE = "acceptance"
    PERFORMANCE = "performance"
    SECURITY = "security"
    REGRESSION = "regression"

class TestStatus(Enum):
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìƒíƒœ."""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"

@dataclass
class TestSpecification:
    """DDD í…ŒìŠ¤íŠ¸ ëª…ì„¸."""
    name: str
    description: str
    test_type: TestType
    requirements: List[str]
    acceptance_criteria: List[str]
    edge_cases: List[str]
    preconditions: List[str] = field(default_factory=list)
    postconditions: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    mock_requirements: Dict[str, Any] = field(default_factory=dict)
    behavior_snapshot: Optional[Dict[str, Any]] = None

@dataclass
class TestCase:
    """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ê°œë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤."""
    id: str
    name: str
    file_path: str
    line_number: int
    specification: TestSpecification
    status: TestStatus
    execution_time: float
    error_message: Optional[str] = None
    coverage_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DDDSession:
    """ì‚¬ì´í´ ì¶”ì ì„ í¬í•¨í•œ DDD ê°œë°œ ì„¸ì…˜."""
    id: str
    project_path: str
    current_phase: DDDPhase
    test_cases: List[TestCase]
    start_time: float
    context7_patterns: Dict[str, Any] = field(default_factory=dict)
    metrics: Dict[str, Any] = field(default_factory=dict)
    behavior_snapshots: Dict[str, Any] = field(default_factory=dict)
```

## DDD Manager êµ¬í˜„

```python
class DDDManager:
    """Context7 í†µí•©ì„ ê°–ì¶˜ ì£¼ìš” DDD ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì."""

    def __init__(self, project_path: str, context7_client=None):
        self.project_path = Path(project_path)
        self.context7 = context7_client
        self.current_session = None
        self.test_history = []

    async def start_ddd_session(
        self, feature_name: str,
        test_types: List[TestType] = None
    ) -> DDDSession:
        """ìƒˆë¡œìš´ DDD ê°œë°œ ì„¸ì…˜ ì‹œì‘."""

        if test_types is None:
            test_types = [TestType.CHARACTERIZATION, TestType.UNIT, TestType.INTEGRATION]

        # ì„¸ì…˜ ìƒì„±
        session = DDDSession(
            id=f"ddd_{feature_name}_{int(time.time())}",
            project_path=str(self.project_path),
            current_phase=DDDPhase.ANALYZE,
            test_cases=[],
            start_time=time.time(),
            context7_patterns={},
            metrics={
                'tests_written': 0,
                'tests_passing': 0,
                'tests_failing': 0,
                'coverage_percentage': 0.0,
                'behaviors_preserved': 0
            },
            behavior_snapshots={}
        )

        self.current_session = session
        return session

    async def run_full_ddd_cycle(
        self, specification: TestSpecification,
        target_function: str = None
    ) -> Dict[str, Any]:
        """ì™„ì „í•œ ANALYZE-PRESERVE-IMPROVE DDD ì‚¬ì´í´ ì‹¤í–‰."""

        cycle_results = {}

        # ANALYZE ë‹¨ê³„
        print("ğŸ” ANALYZE ë‹¨ê³„: ê¸°ì¡´ ì½”ë“œì™€ ë™ì‘ íŒŒì•… ì¤‘...")
        analyze_results = await self._run_analyze_phase(target_function)
        cycle_results['analyze'] = analyze_results
        self.current_session.current_phase = DDDPhase.ANALYZE

        # PRESERVE ë‹¨ê³„
        print("ğŸ§ª PRESERVE ë‹¨ê³„: íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        preserve_results = await self._run_preserve_phase(specification, analyze_results)
        cycle_results['preserve'] = preserve_results
        self.current_session.current_phase = DDDPhase.PRESERVE

        # IMPROVE ë‹¨ê³„
        print("ğŸ”§ IMPROVE ë‹¨ê³„: ë™ì‘ ë³´ì¡´ì„ ìœ ì§€í•˜ë©° ë¦¬íŒ©í† ë§ ì¤‘...")
        improve_results = await self._run_improve_phase(specification)
        cycle_results['improve'] = improve_results
        self.current_session.current_phase = DDDPhase.IMPROVE

        # REVIEW ë‹¨ê³„
        print("âœ… REVIEW ë‹¨ê³„: ìµœì¢… ê²€ì¦ ì¤‘...")
        coverage_results = await self._run_coverage_analysis()
        cycle_results['review'] = {'coverage': coverage_results}
        self.current_session.current_phase = DDDPhase.REVIEW

        return cycle_results

    async def _run_analyze_phase(self, target_function: str = None) -> Dict[str, Any]:
        """ANALYZE: ê¸°ì¡´ ì½”ë“œì™€ ë™ì‘ íŒŒì•…."""

        analysis = {
            'existing_tests': [],
            'code_patterns': [],
            'dependencies': [],
            'behavior_notes': []
        }

        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì°¾ê¸°
        test_files = list(self.project_path.glob("**/test_*.py"))
        analysis['existing_tests'] = [str(f) for f in test_files]

        # ì½”ë“œ êµ¬ì¡° ë¶„ì„
        if target_function:
            analysis['target'] = target_function
            analysis['behavior_notes'].append(f"Analyzing behavior of {target_function}")

        return analysis

    async def _run_preserve_phase(
        self, specification: TestSpecification,
        analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """PRESERVE: ê¸°ì¡´ ë™ì‘ì„ í¬ì°©í•˜ëŠ” íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ ìƒì„±."""

        preserve_results = {
            'characterization_tests_created': 0,
            'behaviors_captured': [],
            'test_files': []
        }

        # ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ ìƒì„±
        for behavior in analysis.get('behavior_notes', []):
            preserve_results['behaviors_captured'].append(behavior)
            preserve_results['characterization_tests_created'] += 1

        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•´ ê¸°ì¤€ì„  í™•ë¦½
        test_results = await self._run_pytest()
        preserve_results['baseline_results'] = test_results

        return preserve_results

    async def _run_improve_phase(self, specification: TestSpecification) -> Dict[str, Any]:
        """IMPROVE: í…ŒìŠ¤íŠ¸ í†µê³¼ë¥¼ ìœ ì§€í•˜ë©° ì½”ë“œ ë¦¬íŒ©í† ë§."""

        improve_results = {
            'improvements_made': [],
            'tests_still_passing': True,
            'refactoring_notes': []
        }

        # ê°œì„  í›„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = await self._run_pytest()
        improve_results['tests_still_passing'] = test_results.get('failed', 0) == 0

        if improve_results['tests_still_passing']:
            self.current_session.metrics['behaviors_preserved'] += 1

        return improve_results

    async def _run_pytest(self) -> Dict[str, Any]:
        """pytest ì‹¤í–‰ í›„ ê²°ê³¼ ë°˜í™˜."""

        try:
            result = subprocess.run(
                [
                    sys.executable, '-m', 'pytest',
                    str(self.project_path),
                    '--tb=short',
                    '-v'
                ],
                capture_output=True,
                text=True,
                cwd=str(self.project_path)
            )

            return self._parse_pytest_output(result.stdout)

        except Exception as e:
            print(f"Error running pytest: {e}")
            return {'error': str(e), 'passed': 0, 'failed': 0}

    def _parse_pytest_output(self, output: str) -> Dict[str, Any]:
        """pytest ì¶œë ¥ íŒŒì‹±."""

        lines = output.split('\n')
        results = {'passed': 0, 'failed': 0, 'skipped': 0, 'total': 0}

        for line in lines:
            if ' passed in ' in line:
                parts = line.split()
                if parts and parts[0].isdigit():
                    results['passed'] = int(parts[0])
                    results['total'] = int(parts[0])
            elif ' passed' in line and ' failed' in line:
                passed_part = line.split(' passed')[0]
                if passed_part.strip().isdigit():
                    results['passed'] = int(passed_part.strip())

                if ' failed' in line:
                    failed_part = line.split(' failed')[0].split(', ')[-1]
                    if failed_part.strip().isdigit():
                        results['failed'] = int(failed_part.strip())

                results['total'] = results['passed'] + results['failed']

        return results

    async def _run_coverage_analysis(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì‹¤í–‰."""

        try:
            result = subprocess.run(
                [
                    sys.executable, '-m', 'pytest',
                    str(self.project_path),
                    '--cov=src',
                    '--cov-report=term-missing'
                ],
                capture_output=True,
                text=True,
                cwd=str(self.project_path)
            )

            return {'coverage_output': result.stdout}

        except Exception as e:
            return {'error': str(e)}

    def get_session_summary(self) -> Dict[str, Any]:
        """í˜„ì¬ DDD ì„¸ì…˜ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°."""

        if not self.current_session:
            return {}

        duration = time.time() - self.current_session.start_time

        return {
            'session_id': self.current_session.id,
            'phase': self.current_session.current_phase.value,
            'duration_seconds': duration,
            'duration_formatted': f"{duration:.1f} seconds",
            'metrics': self.current_session.metrics,
            'test_cases_count': len(self.current_session.test_cases),
            'behaviors_preserved': self.current_session.metrics.get('behaviors_preserved', 0)
        }
```

## ë‹¨ê³„ë³„ ê°€ì´ë“œë¼ì¸

### ANALYZE ë‹¨ê³„
- ê¸°ì¡´ ì½”ë“œ êµ¬ì¡°ì™€ íŒ¨í„´ íŒŒì•…
- ì½”ë“œ ì½ê¸°ë¥¼ í†µí•´ í˜„ì¬ ë™ì‘ ì‹ë³„
- ì˜ì¡´ì„±ê³¼ ì‚¬ì´ë“œ ì´í™íŠ¸ ë¬¸ì„œí™”
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ê³µë°± ë§¤í•‘
- ê¸°ì¡´ ì„¤ê³„ íŒ¨í„´ íŒŒì•…

### PRESERVE ë‹¨ê³„
- ê¸°ì¡´ ë™ì‘ì„ ìœ„í•œ íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ ì‘ì„±
- í˜„ì¬ ë™ì‘ì„ "í™©ê¸ˆ í‘œì¤€"ìœ¼ë¡œ í¬ì°©
- í˜„ì¬ êµ¬í˜„ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•¨ì„ í™•ì¸
- ë°œê²¬ëœ ë™ì‘ ë¬¸ì„œí™”
- ë³µì¡í•œ ì¶œë ¥ì— ëŒ€í•œ ë™ì‘ ìŠ¤ëƒ…ìƒ· ìƒì„±

### IMPROVE ë‹¨ê³„
- í…ŒìŠ¤íŠ¸ë¥¼ ë…¹ìƒ‰ìœ¼ë¡œ ìœ ì§€í•˜ë©° ì½”ë“œ ë¦¬íŒ©í† ë§
- ì‘ê³  ì ì§„ì ì¸ ë³€ê²½ ìˆ˜í–‰
- ë³€ê²½ í›„ë§ˆë‹¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
- ë™ì‘ ë³´ì¡´ ìœ ì§€
- ì„¤ê³„ íŒ¨í„´ ì ì ˆíˆ ì ìš©

### REVIEW ë‹¨ê³„
- ëª¨ë“  íŠ¹ì„±í™” í…ŒìŠ¤íŠ¸ê°€ ì—¬ì „íˆ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸
- ì½”ë“œ í’ˆì§ˆê³¼ ë¬¸ì„œ ê²€í† 
- ë™ì‘ ë³€ê²½ ì—¬ë¶€ í™•ì¸
- ëª…í™•í•œ ë©”ì‹œì§€ë¡œ ë³€ê²½ì‚¬í•­ ì»¤ë°‹
- ìˆ˜í–‰ëœ ê°œì„ ì‚¬í•­ ë¬¸ì„œí™”

## ì‚¬ìš© ì˜ˆì‹œ

```python
# DDD Manager ì´ˆê¸°í™”
ddd_manager = DDDManager(
    project_path="/path/to/project",
    context7_client=context7
)

# DDD ì„¸ì…˜ ì‹œì‘
session = await ddd_manager.start_ddd_session("user_authentication_refactor")

# í…ŒìŠ¤íŠ¸ ëª…ì„¸ ìƒì„±
test_spec = TestSpecification(
    name="test_user_login_behavior_preservation",
    description="Preserve existing login behavior during refactoring",
    test_type=TestType.CHARACTERIZATION,
    requirements=[
        "Existing login flow must continue to work",
        "Error messages should remain consistent"
    ],
    acceptance_criteria=[
        "Valid credentials return user token (existing behavior)",
        "Invalid credentials raise same error messages"
    ],
    edge_cases=[
        "Test with empty email (existing behavior)",
        "Test with empty password (existing behavior)"
    ]
)

# ì „ì²´ DDD ì‚¬ì´í´ ì‹¤í–‰
cycle_results = await ddd_manager.run_full_ddd_cycle(
    specification=test_spec,
    target_function="authenticate_user"
)

# ì„¸ì…˜ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
summary = ddd_manager.get_session_summary()
print(f"Session completed in {summary['duration_formatted']}")
print(f"Behaviors preserved: {summary['behaviors_preserved']}")
```

---

ê´€ë ¨: [í…ŒìŠ¤íŠ¸ ìƒì„±](./test-generation.md) | [í…ŒìŠ¤íŠ¸ íŒ¨í„´](./test-patterns.md)
